---
title: "Modelos Estructurales"
subtitle: "OKONOMETRIC CISE"
author: "William X. Ramos Chucuri"
date: "`r Sys.Date()`"  
format:
  html:
    toc: true
    toc-depth: 3
    code-fold: true
    number-sections: true
    output-file: "index"
  docx: default
  pdf:
    toc: true
    number-sections: true
    df-print: kable
lang: es
params:
  fecha_corte: "2024-12-31"
---

# **1. Modelos Estructurales**

-   Modelos estructurales vs Modelos Estad√≠sticos

-   La estimaci√≥n estructural tiene m√∫ltiples limitaciones.

-   El uso de modelos parsimoniosos tiene un costo, podemos caer el modelos sub-parametrizados, es decir podemos estimar modelos sesgados e inconsistentes. (Condiciones fundamentales en econometr√≠a).

-   La exogeneidad estricta es un supuesto muy restrictivo en un modelo multiecuacional.

-   Los modelos VAR son modelos estad√≠sticos.

    -   El poner restricciones al modelo (Matrices o condiciones- nos ayudar√≠an a transformarlo en un modelo estructural **SVAR.**

-   Los modelos de Equilibrio general son modelos estructurales \| Estos modelos me permiten modelar situaciones como la **pandemia COVID-19.**

    -   Permiten modelar situaciones sobre las cuales no tenemos historia, es decir una estructura consistente.

-   En el Modelo VAR no nos interesan los coeficientes. Hay que recordar que todas las variables son end√≥genas, es decir estan conectados

## 1.2.Modelo VAR(p)

Un modelo autorregresivo vectorial de orden ( p ), denotado como ( VAR(p) ), se define de la siguiente forma:

$X_t = c + A_1 X_{t-1} + A_2 X_{t-2} + \dots + A_p X_{t-p} + \varepsilon_t$

donde:

-   $X_t$ es un vector de variables end√≥genas de dimensi√≥n $K \times 1$.
-   $c$ es un vector de constantes de dimensi√≥n $K \times 1$.
-   $A_i$ son las matrices de coeficientes de dimensi√≥n $K \times K$ para cada rezago $i = 1, 2, \dots, p$.
-   $\varepsilon_t$ son los t√©rminos de error o innovaciones, con media cero y matriz de covarianza:

$\Sigma_u = E[\varepsilon_t \varepsilon_t']$

donde $\Sigma_u$ es sim√©trica y definida positiva.

En forma compacta, el modelo VAR(p) capta la interdependencia din√°mica entre las variables end√≥genas en el tiempo.

## 1.3. Propiedades B√°sicas

-   **Endogeneidad total:** todas las variables son explicadas por sus propios rezagos y los de las dem√°s.

-   **Identificaci√≥n reducida:** los residuos $Œµt$ pueden estar correlacionados, lo que impide inferencias estructurales.

-   **Necesidad de estacionariedad:** Las series deben ser integradas de orden 0 o cointegradas (en cuyo caso se usa VECM).

-   Hay que recordar que al ser integradas I (1) significa que tienen ra√≠z unitaria y son procesos no estacionarios. Si queremos trabajar en modelos multiecuacionales debemos quitar esta ra√≠z unitaria; por eso mencionamos que necesitamos que estas series sean procesos estacionarios.

-   Es decir que su media y varianza esten mas o menos alrededor de cero y constantes en el tiempo.

## 1.4. Proceso de estimaci√≥n

Cada ecuaci√≥n del VAR se estima por M√≠nimos Cuadrados Ordinarios (OLS), dado que los regresores son id√©nticos entre ecuaciones y se espera que el sistema no presente endogeneidad contempor√°nea.

El VAR describe la **din√°mica conjunta de las series** y permite an√°lisis como:

-   impulso-respuesta (IRF)- Una aproximaci√≥n estructural

-   descomposici√≥n de varianza (FEVD),

-   pron√≥sticos conjuntos.

## 1.5. El problema de identificaci√≥n

Un VAR reducido no permite identificar los efectos contempor√°neos ni distinguir causalidad estructural.

## 1.6. Identificaci√≥n en un modelo VAR estructural

Un VAR con ( n ) variables contiene:

$2n^2$

par√°metros en las matrices ( A ) y ( B ).

La matriz de covarianza de los errores, denotada como $\Sigma_u$, solo aporta:

$\frac{n(n + 1)}{2}$

ecuaciones independientes.

Por lo tanto, para lograr la identificaci√≥n del modelo, se requieren al menos:

$\frac{n(n - 1)}{2}$

restricciones adicionales, que generalmente se imponen sobre las matrices ( A ) o ( B ) (dependiendo del tipo de identificaci√≥n: contempor√°nea, de largo plazo, o basada en restricciones de signo).

En un modelo VAR estructural (SVAR), el objetivo es recuperar las relaciones estructurales contempor√°neas entre las variables, es decir, c√≥mo los choques estructurales (innovaciones puras) afectan simult√°neamente a las variables del sistema.

Sin embargo, el modelo estimado directamente (el VAR reducido) solo nos proporciona informaci√≥n sobre la **covarianza observada de los errores**, $\Sigma_u$.\
Esta matriz tiene $n(n + 1)/2$ elementos √∫nicos, ya que es **sim√©trica**.

El problema surge porque las matrices ( A ) y ( B ) contienen en total ( 2n\^2 ) par√°metros desconocidos, mientras que solo contamos con $n(n + 1)/2$ ecuaciones provenientes de $Sigma_u$.\
Esto implica que el sistema est√° **subidentificado**: hay m√°s par√°metros desconocidos que ecuaciones disponibles.

Para resolver este problema, se deben imponer **restricciones adicionales** que permitan identificar cada par√°metro estructural.\
En concreto, se necesitan al menos:

$\frac{n(n - 1)}{2}$

restricciones adicionales para que el n√∫mero de ecuaciones iguale al n√∫mero de par√°metros y el sistema quede **justamente identificado**.

------------------------------------------------------------------------

### üß© Tipos de restricciones comunes

1.  **Restricciones contempor√°neas (modelo tipo AB o recursivo - Cholesky):**\
    Se impone una estructura triangular en ( A ) o ( B ) asumiendo que ciertas variables no responden contempor√°neamente a otras.

2.  **Restricciones de largo plazo (modelo tipo Blanchard-Quah):**\
    Se imponen condiciones sobre la respuesta acumulada a los choques, por ejemplo, que ciertos choques no afectan permanentemente algunas variables.

3.  **Restricciones de signo (modelo de Uhlig):**\
    En lugar de ceros, se imponen signos (+/‚Äì) sobre las respuestas o sobre los impactos contempor√°neos.

------------------------------------------------------------------------

### üß† Ejemplo ilustrativo

Para un VAR de **2 variables (n = 2)**:

-   Par√°metros totales: $2n^2 = 8$
-   Ecuaciones de la covarianza: $n(n + 1)/2 = 3$
-   Restricciones necesarias: $n(n - 1)/2 = 1$

Por tanto, se necesita **al menos una restricci√≥n adicional** (por ejemplo, asumir que una variable no reacciona contempor√°neamente a otra) para lograr la identificaci√≥n.

------------------------------------------------------------------------

üí° **Conclusi√≥n:**

-   La identificaci√≥n en los modelos VAR estructurales es esencial para poder interpretar los choques estimados como ‚Äúcausales‚Äù o ‚Äúestructurales‚Äù.

-   Sin restricciones suficientes, solo observamos correlaciones reducidas, no relaciones estructurales.

## Funciones impulso respuesta

En s√≠ntesis queremos analizar como un shock exogeno afecta a un shock contemporaneo en el momento

**Eigen valores**

Necesitamos que los eigen valores esten dentro del circulo unitario.

Tratamos de modelar relaciones aditivas, es decir que conforme me alejo en el tiempo del shock idiosincr√°tico este se va perdiendo en el tiempo.

## **Modelos parsimoniosos**

En modelos de series temporales es fundamental estimar modelos parsimoniosos, es decir que sean modelos sencillos, el estimar modelos multiecuaciones requiere tener un poder computacional grande.

-   El n√∫mero de rezagos es determinante

-   El n√∫mero de coeficientes de la estimaci√≥n de un modelo VAR en este sistema se incrementa considerablemente por rezagos a incluir.

-   Es fundamental guiarnos en los criterios de informaci√≥n: AIC \| BIC \| Hannan-Quinn para estimar una especificaci√≥n √≥ptima.

-   min (AIC, BIC)

## Ejemplo aplicado: Modelo VAR con tres variables macroecon√≥micas

### üîπ 1. Variables del sistema

Sup√≥n que queremos estudiar la relaci√≥n din√°mica entre:

-   ( Y_t ): **PIB real trimestral** (crecimiento econ√≥mico)\
-   ( X_t ): **Inflaci√≥n trimestral** (IPC)\
-   ( Z_t ): **Tasa de inter√©s de pol√≠tica monetaria**

Nuestro inter√©s es analizar c√≥mo los choques en la tasa de inter√©s afectan la inflaci√≥n y el crecimiento, y viceversa.

------------------------------------------------------------------------

### üîπ 2. Especificaci√≥n del modelo VAR(1)

El modelo VAR(1) se escribe como:

$$
\begin{bmatrix}
Y_t \\
X_t \\
Z_t
\end{bmatrix}
=
\begin{bmatrix}
\psi_Y \\
\psi_X \\
\psi_Z
\end{bmatrix}
+
\begin{bmatrix}
\alpha_{11} & \alpha_{12} & \alpha_{13} \\
\alpha_{21} & \alpha_{22} & \alpha_{23} \\
\alpha_{31} & \alpha_{32} & \alpha_{33}
\end{bmatrix}
\begin{bmatrix}
Y_{t-1} \\
X_{t-1} \\
Z_{t-1}
\end{bmatrix}
+
\begin{bmatrix}
\mu_{Yt} \\
\mu_{Xt} \\
\mu_{Zt}
\end{bmatrix}
$$

------------------------------------------------------------------------

### üîπ 3. Interpretaci√≥n econ√≥mica

Cada ecuaci√≥n representa una variable end√≥gena en funci√≥n de los rezagos de todas las variables:

$$
\begin{aligned}
Y_t &= \psi_Y + \alpha_{11}Y_{t-1} + \alpha_{12}X_{t-1} + \alpha_{13}Z_{t-1} + \mu_{Yt} \\
X_t &= \psi_X + \alpha_{21}Y_{t-1} + \alpha_{22}X_{t-1} + \alpha_{23}Z_{t-1} + \mu_{Xt} \\
Z_t &= \psi_Z + \alpha_{31}Y_{t-1} + \alpha_{32}X_{t-1} + \alpha_{33}Z_{t-1} + \mu_{Zt}
\end{aligned}
$$

**Ejemplos de interpretaci√≥n:**

-   $\alpha_{12}$: mide c√≥mo la inflaci√≥n del trimestre anterior afecta el PIB actual.\
-   $\alpha_{33}$: mide la persistencia de la tasa de inter√©s.\
-   $\alpha_{23}$: mide c√≥mo la tasa de inter√©s pasada influye en la inflaci√≥n presente.

------------------------------------------------------------------------

### üîπ 4. N√∫mero de par√°metros a estimar

La matriz ( A_1 ) tiene:

$$
3 \times 3 = 9 \text{ coeficientes.}
$$

Los interceptos suman 3 par√°metros $(\psi_Y, \psi_X, \psi_Z )$.

**Total en las ecuaciones:**

$$
9 + 3 = 12 \text{ par√°metros.}
$$

La matriz de varianza-covarianza del error es:

$$
\Sigma_{\mu} =
\begin{bmatrix}
\sigma^2_{YY} & \sigma_{YX} & \sigma_{YZ} \\
\sigma_{XY} & \sigma^2_{XX} & \sigma_{XZ} \\
\sigma_{ZY} & \sigma_{ZX} & \sigma^2_{ZZ}
\end{bmatrix}
$$

Esta matriz es **sim√©trica**, por lo que aporta **6 par√°metros adicionales**.

**Total general:**

$$
12 + 6 = 18 \text{ par√°metros a estimar.}
$$

------------------------------------------------------------------------

### üîπ 5. Aumentando la complejidad: VAR(2)

Si a√±adimos un segundo rezago para capturar efectos m√°s lentos:

$$
Y_t = \psi + A_1Y_{t-1} + A_2Y_{t-2} + \mu_t
$$

La nueva matriz ( A_2 ) tambi√©n tiene 9 coeficientes.

Por tanto, el modelo tiene:

$$
3 \text{ (interceptos)} + 9 (A_1) + 9 (A_2) = 21 \text{ par√°metros.}
$$

Si a√±adimos un tercer rezago (VAR(3)):

$$
3 + 9 \times 3 = 30 \text{ par√°metros}
$$

(sin contar los de la matriz de covarianza).

------------------------------------------------------------------------

### üîπ 6. Problema de parsimonia

Mientras m√°s rezagos a√±adimos:

-   Se **incrementa el n√∫mero de par√°metros** a estimar.\
-   Se **reducen los grados de libertad** si la muestra es corta.\
-   Crece el riesgo de **sobreajuste** (el modelo se adapta a los datos hist√≥ricos pero pierde capacidad predictiva).\
-   Aumenta la **complejidad anal√≠tica** (impulso-respuesta, descomposici√≥n de varianza, estabilidad, etc.).

------------------------------------------------------------------------

### ‚úÖ Conclusi√≥n

El modelo VAR permite capturar la interdependencia din√°mica entre variables macroecon√≥micas, pero su uso requiere un equilibrio entre complejidad y parsimonia, especialmente cuando la muestra es limitada o el n√∫mero de variables es grande.

## **Test importantes.**

**Test de Engel y Granger**

-   Estima una regresi√≥n; estima los residuos de la regresi√≥n; especifica que esos residuos son estacionarios, esto mediante los test de Dickey- Fuller.

**Test de Johansen (1988)**

-   Versi√≥n generalizada de los test de Engel y Granger, pero de forma Vectorial Multivariada.

-   Evalua ra√≠z unitaria en vectores ( Sistema de vectores autoregresivos)

**Consideraciones de aplicarlo en un modelo multivariado**

Es posible que tenamos varios vectores de cointegraci√≥n y exista m√°s de un vector de cointegraci√≥n.

**Qu√© es cointegraci√≥n**

Puede que un sistema de ecuaciones de series de tiempo por separado sean no estacionarias, puede existir una combinaci√≥n lineal de ellas que s√≠ sea estacionaria.

**Es relevante?**

Si tenemos dos variables $Y$ y $X$ , llamese el PIB real y el Consumo Privado , en la pr√°ctica, tienden a crecer juntos .

Por lo general, estos son $I(1)$ pero su diferencia puede ser estacionaria

$Consumo - PIB$ pueden ser $I(0)$

Es decir indicar una relaci√≥n estable en el largo plazo.

## **Implicaciones del Modelo VAR**

Cuando las variables estan cointegradas:

-   No se debe estimar un VAR en diferencias, porque se pierde la informaci√≥n del equilibrio de largo plazo.

-   Tampoco un VAR sin restricciones.

-   Soluci√≥n: Estimar un modelo VECM (Modelo de correcci√≥n de Errores) - Es decir;

    -   Din√°mica de corto plazo

    -   Relaci√≥n a largo plazo ( Cointegraci√≥n)

üëâ Si las series son **I(0)** ‚Üí usamos **VAR en niveles**.\

üëâ Si las series son **I(1) y no cointegradas** ‚Üí usamos **VAR en diferencias**.\

üëâ Si las series son **I(1) y cointegradas** ‚Üí usamos **VECM**.

-   **Ejemplo emp√≠co**

```{r}
# =========================================================
# 0) Paquetes y setup
# =========================================================
# install.packages(c("zoo","ggplot2","tseries","vars","urca","forecast","dplyr","tidyr"))
library(zoo)
library(ggplot2)
library(tseries)
library(vars)
library(urca)
library(forecast)
library(dplyr)
library(tidyr)

set.seed(42)

# =========================================================
# 1) Calendario y variables (72 trimestres 2007Q1‚Äì2024Q4)
# =========================================================
T  <- 72
fq <- 4
fechas <- as.yearqtr(seq(from = as.Date("2007-01-01"),
                         by = "quarter", length.out = T))
vars <- c("L_PP","L_PIB","L_GG","L_GC","L_GK","L_IF","L_IVA","L_ICE","L_IR")
k <- length(vars)

# =========================================================
# 2) DGP: I(1) con 2 relaciones de cointegraci√≥n plausibles
# =========================================================
# Œ≤ (columnas = vectores cointegrados)
beta <- matrix(0, nrow = k, ncol = 2, dimnames = list(vars, c("CI_gasto","CI_tributos")))
beta["L_GG","CI_gasto"] <- 1;   beta["L_GC","CI_gasto"] <- -0.7; beta["L_GK","CI_gasto"] <- -0.3
beta["L_IF","CI_tributos"] <- 1; beta["L_IVA","CI_tributos"] <- -0.5; beta["L_IR","CI_tributos"] <- -0.3; beta["L_ICE","CI_tributos"] <- -0.2

# Œ± (velocidades de ajuste)
alpha <- matrix(0, nrow = k, ncol = 2, dimnames = list(vars, c("CI_gasto","CI_tributos")))
alpha["L_GG","CI_gasto"]  <- -0.25
alpha["L_GC","CI_gasto"]  <- -0.10
alpha["L_GK","CI_gasto"]  <- -0.05
alpha["L_IF","CI_tributos"]  <- -0.20
alpha["L_IVA","CI_tributos"] <- -0.10
alpha["L_IR","CI_tributos"]  <- -0.08
alpha["L_ICE","CI_tributos"] <- -0.05
alpha["L_PIB","CI_gasto"]    <- -0.02
alpha["L_PIB","CI_tributos"] <- -0.01
alpha["L_PP",] <- c(0,0)  # petr√≥leo no corrige directamente

# Œì1 (din√°mica de corto plazo sobre ŒîY_{t-1})
Gamma1 <- matrix(0, nrow = k, ncol = k, dimnames = list(vars, vars))
Gamma1["L_PIB","L_PP"] <- 0.10
Gamma1["L_IF","L_PP"]  <- 0.08
Gamma1["L_IF","L_IVA"] <- 0.10; Gamma1["L_IF","L_IR"] <- 0.07; Gamma1["L_IF","L_ICE"] <- 0.05
Gamma1["L_GG","L_PIB"] <- 0.06
Gamma1["L_GC","L_GG"]  <- 0.10; Gamma1["L_GK","L_GG"] <- 0.06

# Œ£ (varianzas-covarianzas de shocks)
Sigma <- diag(c(0.20, 0.18, 0.15, 0.12, 0.12, 0.18, 0.15, 0.12, 0.12))
dimnames(Sigma) <- list(vars, vars)
Sigma["L_PIB","L_PP"] <- Sigma["L_PP","L_PIB"] <- 0.05
Sigma["L_IF","L_PP"]  <- Sigma["L_PP","L_IF"]  <- 0.04
Sigma["L_IF","L_IVA"] <- Sigma["L_IVA","L_IF"] <- 0.06
Sigma["L_GG","L_PIB"] <- Sigma["L_PIB","L_GG"] <- 0.04
Sigma <- (Sigma + t(Sigma))/2
C <- t(chol(Sigma))

# =========================================================
# 3) Simulaci√≥n VECM (ŒîY_t = Œì1 ŒîY_{t-1} + Œ± Œ≤' Y_{t-1} + Œµ_t)
# =========================================================
Y  <- matrix(0, nrow = k, ncol = T, dimnames = list(vars, NULL))
dY <- matrix(0, nrow = k, ncol = T, dimnames = list(vars, NULL))
Y[,1] <- c(4.6, 8.5, 9.2, 8.9, 7.8, 8.6, 7.9, 6.5, 7.6)

for (t in 2:T) {
  eps_t  <- C %*% rnorm(k)
  EC_lag <- t(beta) %*% Y[, t-1]           # (2x9)*(9x1) = (2x1)
  dY[,t] <- Gamma1 %*% dY[,t-1] + alpha %*% EC_lag + eps_t
  Y[,t]  <- Y[,t-1] + dY[,t]
}

```

**Series a modelar**

Identificaci√≥n econ√≥mica que usar√© (Debemos ajustar si quieremos):

-   **Petr√≥leo (L_PP)** es ‚Äúex√≥geno contempor√°neo‚Äù: no recibe shocks contempor√°neos de otras variables (solo su propio shock).

-   **PIB (L_PIB)** puede reaccionar contempor√°neamente a **petr√≥leo**.

-   **Gasto total (L_GG)** reacciona contempor√°neamente al **PIB** (reglas/prociclicidad).

-   **Gasto corriente / de capital (L_GC, L_GK)** reaccionan a **G_G**.

-   **Ingresos (L_IF)** reaccionan a **petr√≥leo** (v√≠a actividad/sector petrolero) y a **impuestos indirectos** (**L_IVA, L_IR, L_ICE**).

-   Para el resto, dejamos par√°metros ‚Äúlibres‚Äù (NA) solo donde tiene sentido; el resto se fijan en 0.

```{r}
Y_ts <- ts(t(Y), start = c(2007,1), frequency = fq); colnames(Y_ts) <- vars
base_wide <- data.frame(
  fecha_q = fechas,
  year    = as.integer(format(fechas, "%Y")),
  quarter = as.integer(cycle(Y_ts)),
  as.data.frame(Y_ts),
  check.names = FALSE
)

base_long <- base_wide |>
  pivot_longer(cols = all_of(vars), names_to = "variable", values_to = "valor")

ggplot(base_long, aes(x = fecha_q, y = valor)) +
  geom_line() + facet_wrap(~ variable, scales = "free_y", ncol = 3) +
  labs(title = "Series simuladas (log)", x = NULL, y = "log") +
  theme_minimal()
```

**Estacionariedad en niveles y en diferencias**

```{r, echo=FALSE, warning=FALSE, message=FALSE}
adf_levels <- sapply(vars, function(v) adf.test(Y_ts[,v])$p.value)
adf_diffs  <- sapply(vars, function(v) adf.test(diff(Y_ts[,v])[-1])$p.value)
cat("\nADF p-valores en niveles:\n"); print(round(adf_levels,4))
cat("\nADF p-valores en diferencias:\n"); print(round(adf_diffs,4))
```

## üìò **1Ô∏è‚É£ Hip√≥tesis del test ADF**

-   **H‚ÇÄ (nula):** La serie tiene una ra√≠z unitaria ‚Üí no estacionaria.

-   **H‚ÇÅ (alternativa):** La serie es estacionaria.

## üìä **2Ô∏è‚É£ Resultados por niveles**

| Variable | p-valor | Interpretaci√≥n                     |
|----------|---------|------------------------------------|
| L_PP     | 0.5077  | No se rechaza H‚ÇÄ ‚Üí no estacionaria |
| L_PIB    | 0.4831  | No estacionaria                    |
| L_GG     | 0.1429  | No estacionaria                    |
| L_GC     | 0.2622  | No estacionaria                    |
| L_GK     | 0.8133  | No estacionaria                    |
| L_IF     | 0.3314  | No estacionaria                    |
| L_IVA    | 0.1580  | No estacionaria                    |
| L_ICE    | 0.6756  | No estacionaria                    |
| L_IR     | 0.1775  | No estacionaria                    |

üîπ **Conclusi√≥n en niveles:**\
Todas las variables tienen p-valores \> 0.05 ‚Üí **no se rechaza la hip√≥tesis nula de ra√≠z unitaria**.\
‚û°Ô∏è **Ninguna serie es estacionaria en niveles.**

## üìà **3Ô∏è‚É£ Resultados en primeras diferencias**

| Variable | p-valor | Interpretaci√≥n                    |
|----------|---------|-----------------------------------|
| L_PP     | 0.0302  | Estacionaria                      |
| L_PIB    | 0.0100  | Estacionaria                      |
| L_GG     | 0.0100  | Estacionaria                      |
| L_GC     | 0.0162  | Estacionaria                      |
| L_GK     | 0.1010  | No estacionaria al 5% (pero casi) |
| L_IF     | 0.1381  | No estacionaria                   |
| L_IVA    | 0.1824  | No estacionaria                   |
| L_ICE    | 0.0965  | No estacionaria (marginal al 10%) |
| L_IR     | 0.0104  | Estacionaria                      |

üîπ **Conclusi√≥n en diferencias:**\
La mayor√≠a de las series

(L_PP, L_PIB, L_GG, L_GC, L_IR)

Son estacionarias en primeras diferencias (p \< 0.05).\
Algunas (L_GK, L_IF, L_IVA, L_ICE) presentan p-valores entre 0.09‚Äì0.18, lo que podr√≠a considerarse estacionarias al 10% o requerir una segunda diferencia o un ajuste de rezagos.

## **4Ô∏è‚É£ Implicaci√≥n para el modelo VAR**

-   Dado que las series no son estacionarias en niveles pero s√≠ lo son en diferencias, se clasifican como I(1) (integradas de orden uno).

-   Esto sugiere que podr√≠a existir cointegraci√≥n entre ellas, por lo cual el paso siguiente ser√≠a aplicar el test de Johansen:

**Seleccionamos los rezagos en niveles (para el Test de Johansen)**

```{r, echo=FALSE, warning=FALSE, message=FALSE}
sel_lvl <- VARselect(Y_ts, lag.max = 4, type = "const")
cat("\nCriterios de informaci√≥n (niveles):\n"); print(sel_lvl$criteria)
p_opt <- as.integer(sel_lvl$selection["AIC(n)"])
Kj <- max(2, p_opt)
cat("\nUsando K =", Kj, "para Johansen.\n")
```

El est√°ndar en econometr√≠a aplicada (ver Johansen, 1991; L√ºtkepohl, 2005) es:

> ‚úÖ ‚ÄúUsar al menos un rezago en diferencias (p‚àí1 ‚â• 1) para capturar la din√°mica de corto plazo.‚Äù
>
> -   Si `p_opt = 1` (por AIC o HQ), se eleva a `K = 2`
>
> -   Si `p_opt = 3`, se usa `K = 3` (sin cambio)
>
> -   Esto garantiza que el VECM tendr√° **al menos un rezago en diferencias (ŒîX‚Çú‚Çã‚ÇÅ)**.
>
> -   $p$ es el n√∫mero de rezagos de un VAR, para eso utilizamos los criterios de informaci√≥n; recordar, el modelo m√°s parsimonioso.
>
> -   $K$ que utiliza el test de Johansen
>
> -   Ojo: En una diferenciaci√≥n podemos omitir este error.

```{r}



j_tr  <- ca.jo(Y_ts, type = "trace", K = Kj, ecdet = "const", spec = "transitory")
j_eig <- ca.jo(Y_ts, type = "eigen", K = Kj, ecdet = "const", spec = "transitory")
cat("\n--- Johansen TRACE ---\n"); print(summary(j_tr))
cat("\n--- Johansen EIGEN ---\n"); print(summary(j_eig))

choose_r_trace <- function(jobj, alpha = 0.05) {
  ts <- jobj@teststat
  cv <- jobj@cval[,"5pct"]
  sum(ts > cv)
}
r <- max(1, choose_r_trace(j_tr, 0.05))   # asegura r‚â•1 para poder estimar VECM
cat("\nRango de cointegraci√≥n estimado (trace, 5%): r =", r, "\n")

```

La salida proviene de la selecci√≥n del n√∫mero √≥ptimo de rezagos $(p)$ en un modelo VAR.

Los criterios de informaci√≥n (AIC, HQ, SC, FPE) comparan la bondad de ajuste y la penalizaci√≥n por complejidad del modelo (n√∫mero de rezagos).

El objetivo es elegir el valor de $(p)$ que minimiza el criterio seleccionado.

-Normalmente **AIC** (Akaike Information Criterion) o **BIC/SC** (Schwarz Criterion).

```{r,echo=FALSE, warning=FALSE, message=FALSE}

j_tr  <- ca.jo(Y_ts, type = "trace", K = Kj, ecdet = "const", spec = "transitory")
j_eig <- ca.jo(Y_ts, type = "eigen", K = Kj, ecdet = "const", spec = "transitory")
cat("\n--- Johansen TRACE ---\n"); print(summary(j_tr))
cat("\n--- Johansen EIGEN ---\n"); print(summary(j_eig))

```

## Interpretaci√≥n del Test de Johansen

### 1Ô∏è‚É£ Prueba de la traza (Trace Test)

El test **trace** contrasta la hip√≥tesis nula de que existen como m√°ximo *r* vectores de cointegraci√≥n frente a la alternativa de que hay m√°s.\
El resultado fue:

| Hip√≥tesis nula | Estad√≠stico | Valor cr√≠tico (5%) | Decisi√≥n |
|----|----|----|----|
| r = 0 | **220.68** | 202.92 | **Rechazar H‚ÇÄ** ‚Üí existe al menos 1 relaci√≥n de cointegraci√≥n |
| r ‚â§ 1 | **158.09** | 165.58 | No se rechaza H‚ÇÄ |
| r ‚â§ 2 | 117.22 | 131.70 | No se rechaza H‚ÇÄ |
| ... | ... | ... | ... |

‚úÖ **Conclusi√≥n:** se identifica **una relaci√≥n de cointegraci√≥n** entre las variables analizadas (*L_PP, L_PIB, L_GG, L_GC, L_GK, L_IF, L_IVA, L_ICE, L_IR*).\
Esto implica que las series son integradas de orden uno (I(1)) y mantienen una relaci√≥n de equilibrio a largo plazo.

------------------------------------------------------------------------

### 2Ô∏è‚É£ Prueba del m√°ximo eigenvalor (Eigenvalue Test)

El test **maximum eigenvalue (Œªmax)** contrasta *r* vectores frente a la alternativa de *r+1*.\
El resultado fue:

| Hip√≥tesis nula | Estad√≠stico | Valor cr√≠tico (5%) | Decisi√≥n         |
|----------------|-------------|--------------------|------------------|
| r = 0          | **62.58**   | 57.42              | **Rechazar H‚ÇÄ**  |
| r ‚â§ 1          | **40.88**   | 52.00              | No se rechaza H‚ÇÄ |
| ...            | ...         | ...                | ...              |

‚úÖ **Conclusi√≥n:** tambi√©n indica **una sola relaci√≥n de cointegraci√≥n**, coherente con el test trace.

------------------------------------------------------------------------

### 3Ô∏è‚É£ Interpretaci√≥n de los eigenvectores (Œ≤)

Los **eigenvectores** representan las **relaciones de cointegraci√≥n**, es decir, las combinaciones lineales estacionarias entre las variables en niveles.

\
Por ejemplo, el primer vector (normalizado respecto a `L_PP.l1`) puede escribirse como:

\[ L_PP = 0.73 L_PIB - 0.28 L_GG - 0.18 L_GC - 0.25 L_GK - 0.12 L_IF + 0.58 L_IVA + 0.76 L_ICE + 1.32 L_IR - 21.36 \]

üß© **Interpretaci√≥n econ√≥mica:** - Un aumento del **PIB** (+0.73) est√° asociado con un incremento de los **precios p√∫blicos (L_PP)** en el largo plazo.\
- **Gasto de gobierno (L_GG)** y **consumo p√∫blico (L_GC)** tienen relaci√≥n negativa, sugiriendo efectos de equilibrio fiscal o restricciones presupuestarias.\
- El **impuesto al valor agregado (L_IVA)** y el **impuesto a consumos especiales (L_ICE)** tienen signo positivo, reflejando que mayores recaudaciones se asocian con mayores ingresos totales.\
- La **constante negativa (-21.36)** representa el t√©rmino de equilibrio de largo plazo.

------------------------------------------------------------------------

### 4Ô∏è‚É£ Interpretaci√≥n de los weights (Œ±)

Los **weights** (o matriz de carga Œ±) indican **la velocidad de ajuste al equilibrio** ante desviaciones de largo plazo.\
Valores altos (en valor absoluto) implican una r√°pida correcci√≥n.

Por ejemplo: - `L_PP.d` tiene una carga de `-0.17`, indicando que los precios p√∫blicos se ajustan moderadamente hacia el equilibrio.\
- `L_GC.d` con `-0.20` sugiere que el gasto corriente responde m√°s r√°pido a los desequilibrios.\
- `L_PIB.d` y `L_IVA.d` presentan coeficientes peque√±os, indicando menor sensibilidad o ajuste lento.

------------------------------------------------------------------------

### 5Ô∏è‚É£ Conclusi√≥n general

El sistema presenta **una relaci√≥n de cointegraci√≥n**, lo que implica que las variables fiscales y macroecon√≥micas (PIB, gasto, impuestos) **comparten un equilibrio de largo plazo**.\
Las desviaciones de corto plazo se corrigen gradualmente, especialmente a trav√©s de los ajustes en precios p√∫blicos y gasto corriente.

------------------------------------------------------------------------

**Resumen interpretativo**

| Elemento | Descripci√≥n | Conclusi√≥n |
|----|----|----|
| Test Trace | Eval√∫a n√∫mero de relaciones de cointegraci√≥n | 1 vector cointegrante |
| Test Eigenvalue | Eval√∫a significancia marginal de cada ra√≠z caracter√≠stica | 1 vector cointegrante |
| Eigenvectors (Œ≤) | Relaci√≥n de equilibrio entre las variables | Existe relaci√≥n estable entre ingresos, gasto y PIB |
| Weights (Œ±) | Velocidad de ajuste | Ajuste moderado en L_PP y L_GC |
| Conclusi√≥n econ√≥mica | Equilibrio fiscal de largo plazo con desajustes transitorios | Evidencia de cointegraci√≥n macrofiscal |

------------------------------------------------------------------------

```{r}
choose_r_trace <- function(jobj, alpha = 0.05) {
  ts <- jobj@teststat
  cv <- jobj@cval[,"5pct"]
  sum(ts > cv)
}
r <- max(1, choose_r_trace(j_tr, 0.05))   # asegura r‚â•1
cat("\nRango de cointegraci√≥n estimado (trace, 5%): r =", r, "\n")

```

Esto significa que el test Johansen de traza encontr√≥ una sola relaci√≥n de cointegraci√≥n entre las variables (r = 1), al nivel de significancia del 5%.

**Dado que el modelo tiene cointegraci√≥n**

Procedemos a estimar un modelo de Vectores cointegrados

```{r}
vecm_fit <- cajorls(j_tr, r = r)
cat("\n--- Beta (vectores cointegrados estimados) ---\n")
print(vecm_fit$beta)

var_from_vecm <- vec2var(j_tr, r = r)  
```

## ¬øQu√© es ese Œ≤ (‚Äúect1‚Äù)?

Con ( r = 1 ), hay una **relaci√≥n de largo plazo** entre las 9 variables:

$$
X_t = (L_{PP}, \; L_{PIB}, \; L_{GG}, \; L_{GC}, \; L_{GK}, \; L_{IF}, \; L_{IVA}, \; L_{ICE}, \; L_{IR})'
$$

El objeto `vecm_fit$beta` entrega el vector $\beta$ tal que el **t√©rmino de correcci√≥n de error (ECT)** se define como:

$$
ECT_t = \beta' X_{t-1}
$$

En nuestro ejemplo emp√≠rico:

$$
\begin{aligned}
ECT_t &= 1 \cdot L_{PP,t-1}
+ 0.7325 \cdot L_{PIB,t-1}
- 0.2779 \cdot L_{GG,t-1}
- 0.1847 \cdot L_{GC,t-1}
- 0.2481 \cdot L_{GK,t-1} \\
&\quad - 0.1274 \cdot L_{IF,t-1}
+ 0.5791 \cdot L_{IVA,t-1}
+ 0.7588 \cdot L_{ICE,t-1}
+ 1.3175 \cdot L_{IR,t-1}
- 21.3593
\end{aligned}
$$

------------------------------------------------------------------------

### üîπ Interpretaci√≥n del ECT

En **equilibrio de largo plazo**, se cumple que:

$$
ECT_t = 0
$$

Cuando $ECT_t \neq 0$, significa que existe un **desv√≠o del equilibrio**.

La fila `constant = -21.3593` corresponde al **intercepto de la relaci√≥n de largo plazo**, una constante restringida al **espacio cointegrante**, debido a la opci√≥n `ecdet = "const"` en la estimaci√≥n del modelo.

El hecho de que \$L\_{PP,t-1} = 1 \$ indica que **Œ≤ est√° normalizado sobre ( L\_{PP} )**, aunque podr√≠as normalizar sobre otra variable si deseas interpretar las **elasticidades** de manera m√°s conveniente.

------------------------------------------------------------------------

### üîπ ¬øC√≥mo se usa el ECT en el VECM?

El modelo de correcci√≥n de errores (VECM) para cada variable en diferencias incluye el t√©rmino:

$$
\alpha \cdot ECT_{t-1}
$$

donde $\alpha$ representa los **loadings** o coeficientes de ajuste.

Estos coeficientes $\alpha$ miden **qu√© tan fuerte y en qu√© direcci√≥n** cada variable corrige los desv√≠os del equilibrio de largo plazo.

Por ejemplo, en la ecuaci√≥n de:

$$
\Delta L_{PP}
$$

si el coeficiente asociado a $ECT_{t-1}$ es decir, su $\alpha$ es **negativo y estad√≠sticamente significativo**, implica que:

-   Cuando ( ECT\_{t-1} \> 0 ) (es decir, el precio est√° **por encima del equilibrio**),\
-   entonces $\Delta L_{PP}$ tender√° a ser **negativo**,\
-   lo cual **corrige el desv√≠o** y restablece el equilibrio.

------------------------------------------------------------------------

### ‚úÖ En resumen

-   $\beta$: define la **relaci√≥n de cointegraci√≥n** (equilibrio de largo plazo).\
-   $ECT_t$: mide el **grado de desviaci√≥n** respecto a ese equilibrio.\
-   $\alpha$: indica **la velocidad de ajuste** de cada variable hacia el equilibrio.

## **Diagnosticos**

```{r}

cat("\n--- Portmanteau ---\n"); print(serial.test(var_from_vecm, lags.pt = 12, type = "PT.asymptotic"))
cat("\n--- ARCH ---\n"); print(arch.test(var_from_vecm, lags.multi = 5))
cat("\n--- Normalidad ---\n"); print(normality.test(var_from_vecm))
```

## üîπ Diagn√≥sticos del modelo VAR / VECM

------------------------------------------------------------------------

### 1Ô∏è‚É£. Portmanteau test ‚Üí Autocorrelaci√≥n serial

**Hip√≥tesis:**

$$
\begin{aligned}
H_0 &: \text{No hay autocorrelaci√≥n serial en los residuos.} \\
H_1 &: \text{Existe autocorrelaci√≥n (dependencia entre errores rezagados).}
\end{aligned}
$$

**Resultado:**

$$
\chi^2 = 854.21, \quad df = 819, \quad p\text{-value} = 0.191
$$

**Interpretaci√≥n:**

-   $p\text{-value} = 0.191 > 0.05 \Rightarrow$ **No se rechaza ( H_0 )**.\
-   Los residuos **no presentan autocorrelaci√≥n significativa**.

‚úÖ Esto indica que el modelo **captura adecuadamente la din√°mica temporal**, es decir, los rezagos elegidos son suficientes.

**Comentario para clase:**

> ‚ÄúEl test Portmanteau verifica que los residuos del VAR no tengan correlaci√≥n serial.\
> Si el p-valor es alto, el modelo est√° bien especificado en t√©rminos de din√°mica temporal.‚Äù

------------------------------------------------------------------------

### 2Ô∏è‚É£. ARCH test ‚Üí Heterocedasticidad condicional

**Hip√≥tesis:**

$$
\begin{aligned}
H_0 &: \text{No hay efectos ARCH (varianza constante en el tiempo).} \\
H_1 &: \text{Existe heterocedasticidad condicional (la varianza de los errores cambia con el tiempo).}
\end{aligned}
$$

**Resultado:**

$$
\chi^2 = 2925, \quad df = 10125, \quad p\text{-value} = 1.000
$$

**Interpretaci√≥n:**

-   $p\text{-value} = 1.000 > 0.05 \Rightarrow$ **No se rechaza ( H_0 )**.\
-   No hay evidencia de efectos ARCH ‚Üí la varianza de los residuos parece **estable**.

**Comentario:**

> ‚ÄúEl test ARCH comprueba si existe heterocedasticidad en los errores.\
> En este caso, los resultados muestran **homocedasticidad**, es decir, el modelo no presenta volatilidad cambiante.‚Äù

------------------------------------------------------------------------

### 3Ô∏è‚É£. Normalidad multivariada (Jarque‚ÄìBera test)

**Hip√≥tesis:**

$$
\begin{aligned}
H_0 &: \text{Los residuos son normales multivariados.} \\
H_1 &: \text{Los residuos no siguen una distribuci√≥n normal.}
\end{aligned}
$$

**Resultados:**

$$
\chi^2 = 12.03, \quad df = 18, \quad p\text{-value} = 0.8457
$$

**Desglose:**

-   **Skewness:** ( p = 0.5993 )\
-   **Kurtosis:** ( p = 0.8623 )

**Interpretaci√≥n:**

-   Los **p-valores son muy altos**, por lo tanto **no se rechaza ( H_0 )**.\
-   Los residuos cumplen el supuesto de **normalidad**, tanto en simetr√≠a como en curtosis.

**Comentario:**

> ‚ÄúLa normalidad de los residuos es importante para la validez de los intervalos de confianza\
> y las pruebas de hip√≥tesis en el VAR. En este caso, los residuos se distribuyen normalmente.‚Äù

------------------------------------------------------------------------

### üîπ ‚úÖ Conclusi√≥n general

| **Test** | **Hip√≥tesis nula** | **p-value** | **Decisi√≥n** | **Conclusi√≥n** |
|----|----|----|----|----|
| Portmanteau | No autocorrelaci√≥n | 0.191 | No se rechaza | Residuos no autocorrelados |
| ARCH | Homocedasticidad | 1.000 | No se rechaza | Varianza constante |
| Normalidad (JB) | Normalidad multivariada | 0.8457 | No se rechaza | Residuos normales |

------------------------------------------------------------------------

**Conclusi√≥n:**

Los resultados de las pruebas indican que el **modelo VAR/VECM est√° bien especificado**:

-   No hay autocorrelaci√≥n serial.\
-   Los errores son homoced√°sticos.\
-   Los residuos siguen una distribuci√≥n aproximadamente normal.

‚úÖ Esto valida la **consistencia de las inferencias**, como los an√°lisis de **impulso‚Äìrespuesta**, **descomposici√≥n de varianza** y **efectos de transmisi√≥n**.

**Estimaci√≥n Estabilidad**

```{r}
var_levels <- VAR(Y_ts, p = Kj, type = "const")
mod_roots <- roots(var_levels, modulus = TRUE)
cat("\nM√≥dulos de autovalores (VAR en niveles, p=Kj):\n"); print(round(sort(mod_roots, decreasing = TRUE), 4))
plot(roots(var_levels))   # puntos dentro del c√≠rculo => estable

```

Son los **autovalores (roots)** de un modelo **VAR en niveles**, y su **verificaci√≥n de estabilidad**, que es un paso fundamental antes de interpretar los resultados o generar funciones impulso‚Äìrespuesta (IRF).

-   Calculas los **autovalores del polinomio caracter√≠stico** del VAR.

-   Los **modos (m√≥dulos)** de esos autovalores determinan la **estabilidad** del sistema din√°mico

-   Un modelo VAR es **estable** (o estacionario en niveles) si **todos los autovalores tienen m√≥dulo menor que 1**.

-   Esto equivale a decir que las ra√≠ces del polinomio caracter√≠stico est√°n **fuera del c√≠rculo unitario** (en el plano complejo).

-   Todos los valores son **menores que 1**, lo que implica:

    ‚úÖ **El VAR es estable**\
    ‚úÖ **El sistema es estacionario en niveles (no explosivo)**\
    ‚úÖ Se pueden calcular funciones impulso‚Äìrespuesta (IRF) y descomposici√≥n de varianza sin problemas.

## VAR equivalente del VECM (para IRF/FEVD/forecast)

```{r}
dY_ts <- na.omit(diff(Y_ts))
sel_d <- VARselect(dY_ts, lag.max = 4, type = "const")
p_d <- ifelse(is.na(as.integer(sel_d$selection["AIC(n)"])), 2L, as.integer(sel_d$selection["AIC(n)"]))
cat("\nVAR(Œî) seleccionado (AIC): p_d =", p_d, "\n")
var_d <- VAR(dY_ts, p = p_d, type = "const")
stb_diff <- stability(var_d, type = "OLS-CUSUM")

```

-   Estima el modelo VAR con las diferencias y el lag p=1.

-   Incluye una constante (`type = "const"`) para capturar el promedio de crecimiento.

üîπ Ahora `var_d` contiene el modelo listo para IRF, FEVD, forecast y pruebas de estabilidad.

-   Aplica el test **OLS-CUSUM** (Brown, Durbin y Evans, 1975) a los residuos recursivos.

    Eval√∫a si los coeficientes del VAR son **estables en el tiempo**.

    Si las l√≠neas del gr√°fico CUSUM se mantienen **dentro de las bandas cr√≠ticas (95%)**, el modelo es **estable**.

```{r}
png("stability_VAR_diff_CUSUM.png", width = 1400, height = 900, res = 150)
par(mar = c(4, 4, 2, 1))
plot(stb_diff)
dev.off()
cat("Gr√°fico guardado en: stability_VAR_diff_CUSUM.png\n")
```

Esto es el **VAR equivalente del VECM**, ya que si tus series son I(1) y cointegradas (r=1), el VECM y el VAR(Œî) contienen la misma din√°mica de corto plazo.

-   Calcula las **diferencias primeras** de cada serie

-   `na.omit()` elimina los valores perdidos de la primera observaci√≥n.

üîπ Resultado: `dy_ts` contiene la versi√≥n estacionaria (en diferencias) de tu sistema multivariado.

**IRF**

```{r}

irf_pp_pib <- irf(var_from_vecm, impulse = "L_PP", response = "L_PIB",
                  n.ahead = 12, boot = TRUE, ci = 0.95)
plot(irf_pp_pib)

fevd_v <- fevd(var_from_vecm, n.ahead = 12)


# Cierra cualquier gr√°fico abierto
while (!is.null(dev.list())) dev.off()

# Exporta el FEVD a un PNG
png("FEVD_VECM.png", width = 1400, height = 900, res = 150)
par(mar = c(4,4,2,1))  # M√°rgenes reducidas
plot(fevd_v)
dev.off()

cat("‚úÖ Gr√°fico FEVD guardado como 'FEVD_VECM.png' en tu directorio de trabajo.\n")


```

## üîπ Funci√≥n `irf()` ‚Äî Respuestas Impulso‚ÄìRespuesta (IRF)

### ¬øQu√© hace?

La funci√≥n `irf()` genera las **respuestas impulso‚Äìrespuesta** (*Impulse Response Functions*) del modelo `var_from_vecm`, que proviene del **VECM cointegrado**.

Estas funciones permiten analizar c√≥mo una **perturbaci√≥n (shock)** en una variable afecta a las dem√°s en el tiempo.

------------------------------------------------------------------------

### üîπ Par√°metros principales

| **Argumento** | **Descripci√≥n** |
|----|----|
| `impulse = "L_PP"` | Variable que recibe el **shock ex√≥geno** (perturbaci√≥n). |
| `response = "L_PIB"` | Variable cuya **respuesta** se observa. |
| `n.ahead = 12` | Horizonte temporal de **12 per√≠odos** (por ejemplo, trimestres o meses). |
| `boot = TRUE` | Activa el **m√©todo bootstrap** para obtener intervalos de confianza. |
| `ci = 0.95` | Establece un **intervalo de confianza del 95%**. |

------------------------------------------------------------------------

### üîπ Resultado e interpretaci√≥n

El gr√°fico que produce `irf()` muestra c√≥mo un **shock inesperado** en ( L\_{PP} ) (por ejemplo, precios o pol√≠tica p√∫blica) afecta al **PIB** (( L\_{PIB} )) a lo largo de 12 per√≠odos.

-   **Eje X:** Horizonte temporal (n√∫mero de per√≠odos).\
-   **Eje Y:** Magnitud y signo de la respuesta de la variable dependiente.

Las **bandas grises** o **l√≠neas discontinuas** representan los **intervalos de confianza al 95%**:

$$
\text{Si la banda incluye el cero} \Rightarrow \text{la respuesta no es estad√≠sticamente significativa.}
$$

$$
\text{Si la banda se mantiene por encima o por debajo del cero} \Rightarrow \text{la respuesta s√≠ es significativa.}
$$

------------------------------------------------------------------------

### üîπ üìà Ejemplo de interpretaci√≥n

> ‚ÄúUn aumento inesperado en ( L\_{PP} ) genera un **incremento transitorio** en ( L\_{PIB} ) durante los primeros **3 per√≠odos**, que luego **se disipa** gradualmente.‚Äù

------------------------------------------------------------------------

```{r}
# IRF: L_PP ‚Üí L_PIB
png("IRF_L_PP_to_L_PIB.png", width = 1400, height = 900, res = 150)
par(mar = c(4,4,2,1))
plot(irf_pp_pib)
dev.off()

cat("‚úÖ Gr√°fico IRF guardado como 'IRF_L_PP_to_L_PIB.png'\n")

```

## **Modelo SVAR**

```{r}
# =========================================================
# 0) Paquetes y setup
# =========================================================
# install.packages(c("zoo","ggplot2","tseries","vars","urca","forecast","dplyr","tidyr"))
library(zoo)
library(ggplot2)
library(tseries)
library(vars)
library(urca)
library(forecast)
library(dplyr)
library(tidyr)

set.seed(42)

# =========================================================
# 1) Calendario y variables (72 trimestres 2007Q1‚Äì2024Q4)
# =========================================================
T  <- 72
fq <- 4
fechas <- as.yearqtr(seq(from = as.Date("2007-01-01"),
                         by = "quarter", length.out = T))
vars <- c("L_PP","L_PIB","L_GG","L_GC","L_GK","L_IF","L_IVA","L_ICE","L_IR")
k <- length(vars)

# =========================================================
# 2) DGP: I(1) con 2 cointegraciones plausibles (para simular)
# =========================================================
beta <- matrix(0, nrow = k, ncol = 2, dimnames = list(vars, c("CI_gasto","CI_tributos")))
beta["L_GG","CI_gasto"] <- 1;   beta["L_GC","CI_gasto"] <- -0.7; beta["L_GK","CI_gasto"] <- -0.3
beta["L_IF","CI_tributos"] <- 1; beta["L_IVA","CI_tributos"] <- -0.5; beta["L_IR","CI_tributos"] <- -0.3; beta["L_ICE","CI_tributos"] <- -0.2

alpha <- matrix(0, nrow = k, ncol = 2, dimnames = list(vars, c("CI_gasto","CI_tributos")))
alpha["L_GG","CI_gasto"]  <- -0.25
alpha["L_GC","CI_gasto"]  <- -0.10
alpha["L_GK","CI_gasto"]  <- -0.05
alpha["L_IF","CI_tributos"]  <- -0.20
alpha["L_IVA","CI_tributos"] <- -0.10
alpha["L_IR","CI_tributos"]  <- -0.08
alpha["L_ICE","CI_tributos"] <- -0.05
alpha["L_PIB","CI_gasto"]    <- -0.02
alpha["L_PIB","CI_tributos"] <- -0.01
alpha["L_PP",] <- c(0,0)  # petr√≥leo no corrige directamente

Gamma1 <- matrix(0, nrow = k, ncol = k, dimnames = list(vars, vars))
Gamma1["L_PIB","L_PP"] <- 0.10
Gamma1["L_IF","L_PP"]  <- 0.08
Gamma1["L_IF","L_IVA"] <- 0.10; Gamma1["L_IF","L_IR"] <- 0.07; Gamma1["L_IF","L_ICE"] <- 0.05
Gamma1["L_GG","L_PIB"] <- 0.06
Gamma1["L_GC","L_GG"]  <- 0.10
Gamma1["L_GK","L_GG"]  <- 0.06

Sigma <- diag(c(0.20, 0.18, 0.15, 0.12, 0.12, 0.18, 0.15, 0.12, 0.12))
dimnames(Sigma) <- list(vars, vars)
Sigma["L_PIB","L_PP"] <- Sigma["L_PP","L_PIB"] <- 0.05
Sigma["L_IF","L_PP"]  <- Sigma["L_PP","L_IF"]  <- 0.04
Sigma["L_IF","L_IVA"] <- Sigma["L_IVA","L_IF"] <- 0.06
Sigma["L_GG","L_PIB"] <- Sigma["L_PIB","L_GG"] <- 0.04
Sigma <- (Sigma + t(Sigma))/2
C <- t(chol(Sigma))

# =========================================================
# 3) Simulaci√≥n VECM (ŒîY_t = Œì1 ŒîY_{t-1} + Œ± Œ≤' Y_{t-1} + Œµ_t)
# =========================================================
Y  <- matrix(0, nrow = k, ncol = T, dimnames = list(vars, NULL))
dY <- matrix(0, nrow = k, ncol = T, dimnames = list(vars, NULL))
Y[,1] <- c(4.6, 8.5, 9.2, 8.9, 7.8, 8.6, 7.9, 6.5, 7.6)

for (t in 2:T) {
  eps_t  <- C %*% rnorm(k)
  EC_lag <- t(beta) %*% Y[, t-1]           # (2x9)*(9x1) = (2x1)
  dY[,t] <- Gamma1 %*% dY[,t-1] + alpha %*% EC_lag + eps_t
  Y[,t]  <- Y[,t-1] + dY[,t]
}

Y_ts <- ts(t(Y), start = c(2007,1), frequency = fq); colnames(Y_ts) <- vars


```

```{r}
dY_ts <- na.omit(diff(Y_ts))
sel_d <- VARselect(dY_ts, lag.max = 4, type = "const")
p_d <- ifelse(is.na(as.integer(sel_d$selection["AIC(n)"])), 2L, as.integer(sel_d$selection["AIC(n)"]))
cat("Rezagos VAR(Œî) (AIC): p_d =", p_d, "\n")

var_d <- VAR(dY_ts, p = p_d, type = "const")   # <- 'varest'
cat("\nResumen VAR(Œî):\n"); print(summary(var_d))

# Diagn√≥sticos r√°pidos
cat("\nPortmanteau (autocorrelaci√≥n):\n"); print(serial.test(var_d, lags.pt = 12, type = "PT.asymptotic"))
cat("\nARCH (heterocedasticidad):\n");     print(arch.test(var_d, lags.multi = 5))
cat("\nNormalidad (Jarque-Bera multivariante):\n"); print(normality.test(var_d))

# Estabilidad (ra√≠ces del VAR(Œî))
mod_roots <- roots(var_d, modulus = TRUE)
cat("\nM√≥dulos de autovalores (VAR(Œî)):\n"); print(round(sort(mod_roots, decreasing = TRUE), 4))


```

```{r}
vnames_d <- colnames(var_d$y)
K <- length(vnames_d)

# A: 1 en diagonal; 0 = cero fijo; NA = libre
Amat_d <- diag(1, K); dimnames(Amat_d) <- list(vnames_d, vnames_d)

# ŒîL_PP (shock de petr√≥leo) NO recibe contempor√°neos del resto:
Amat_d["L_PP", setdiff(vnames_d, "L_PP")] <- 0

# Canales contempor√°neos plausibles (libres)
Amat_d["L_PIB","L_PP"] <- NA      # petr√≥leo ‚Üí PIB
Amat_d["L_GG","L_PIB"] <- NA      # PIB ‚Üí gasto total
Amat_d["L_GC","L_GG"]  <- NA      # gasto total ‚Üí gasto corriente
Amat_d["L_GK","L_GG"]  <- NA      # gasto total ‚Üí gasto capital
Amat_d["L_IF","L_PP"]  <- NA      # petr√≥leo ‚Üí ingresos fiscales
Amat_d["L_IF","L_IVA"] <- NA      # IVA ‚Üí ingresos fiscales
Amat_d["L_IF","L_IR"]  <- NA      # IR ‚Üí ingresos fiscales
Amat_d["L_IF","L_ICE"] <- NA      # ICE ‚Üí ingresos fiscales
# (todo lo no especificado queda en 0; ajusta NA/0 seg√∫n tu narrativa)

# B: diagonal (NA para las sd de los shocks), 0 fuera
Bmat_d <- diag(NA, K); dimnames(Bmat_d) <- list(vnames_d, vnames_d)
Bmat_d[row(Bmat_d) != col(Bmat_d)] <- 0

# Estimar SVAR(Œî)
svar_d <- SVAR(var_d, Amat = Amat_d, Bmat = Bmat_d, estmethod = "direct")
cat("\nSVAR(Œî) estimado con √©xito.\n")

```

```{r}
while (!is.null(dev.list())) dev.off()

# IRF: shock petr√≥leo ‚Üí PIB
irf_pp_pib_d <- irf(svar_d, impulse = "L_PP", response = "L_PIB",
                    n.ahead = 12, boot = TRUE, ci = 0.95)
png("IRF_SVAR_DIFF_LPP_to_LPIB.png", width = 1400, height = 900, res = 150)
par(mar = c(4,4,2,1)); plot(irf_pp_pib_d); dev.off()
cat("‚úÖ Guardado: IRF_SVAR_DIFF_LPP_to_LPIB.png\n")

# IRF: shock petr√≥leo ‚Üí ingresos fiscales
irf_pp_if_d <- irf(svar_d, impulse = "L_PP", response = "L_IF",
                   n.ahead = 12, boot = TRUE, ci = 0.95)
png("IRF_SVAR_DIFF_LPP_to_LIF.png", width = 1400, height = 900, res = 150)
par(mar = c(4,4,2,1)); plot(irf_pp_if_d); dev.off()
cat("‚úÖ Guardado: IRF_SVAR_DIFF_LPP_to_LIF.png\n")

# FEVD (12 pasos)
fevd_d <- fevd(svar_d, n.ahead = 12)
png("FEVD_SVAR_DIFF_12.png", width = 1400, height = 900, res = 150)
par(mar = c(4,4,2,1)); plot(fevd_d); dev.off()
cat("‚úÖ Guardado: FEVD_SVAR_DIFF_12.png\n")

# IRFs de petr√≥leo hacia todas las respuestas (lote)
responses <- c("L_PIB","L_GG","L_GC","L_GK","L_IF","L_IVA","L_ICE","L_IR")
for (resp in responses) {
  f <- irf(svar_d, impulse = "L_PP", response = resp, n.ahead = 12, boot = TRUE, ci = 0.95)
  fn <- paste0("IRF_SVAR_DIFF_LPP_to_", resp, ".png")
  png(fn, width = 1400, height = 900, res = 150)
  par(mar = c(4,4,2,1)); plot(f); dev.off()
  cat("‚úÖ Guardado: ", fn, "\n", sep = "")
}

cat("\n=== FIN: SVAR en diferencias (estacionario) con gr√°ficos exportados ===\n")
```

Forecast

```{r}
# ===============================
# FORECAST con VAR en diferencias
# ===============================

# Horizonte de pron√≥stico (trimestres)
h <- 8  # 2 a√±os

# 1) Pron√≥stico de diferencias (Œîlog) con el VAR estacionario
fc_d <- predict(var_d, n.ahead = h, ci = 0.95)

# Extraer medias, l√≠mites inferior/superior por variable
vnames_d <- colnames(var_d$y)
dhat_mean <- sapply(vnames_d, function(v) fc_d$fcst[[v]][, "fcst"])
dhat_low  <- sapply(vnames_d, function(v) fc_d$fcst[[v]][, "lower"])
dhat_high <- sapply(vnames_d, function(v) fc_d$fcst[[v]][, "upper"])

# 2) Reconstruir niveles (logs) desde el √∫ltimo dato observado
last_level <- as.numeric(tail(Y_ts, 1))   # vector de 9 niveles (logs) en 2024Q4
names(last_level) <- colnames(Y_ts)

# Funci√≥n auxiliar: acumula difs para pasar a niveles
rebuild_levels <- function(last_level, diffs_mat) {
  # diffs_mat: h x k (Œîlog pronosticadas)
  lev <- matrix(NA_real_, nrow = nrow(diffs_mat), ncol = ncol(diffs_mat))
  colnames(lev) <- colnames(diffs_mat)
  prev <- last_level
  for (t in 1:nrow(diffs_mat)) {
    lev[t, ] <- prev + diffs_mat[t, ]
    prev     <- lev[t, ]
  }
  lev
}

# Niveles pronosticados (punto), y bandas aprox. por acumulaci√≥n (did√°ctico)
lev_hat_mean <- rebuild_levels(last_level, dhat_mean)
lev_hat_low  <- rebuild_levels(last_level, dhat_low)
lev_hat_high <- rebuild_levels(last_level, dhat_high)

# 3) Construir fechas futuras (trimestres posteriores a 2024Q4)
#    Reusamos la secuencia "fechas" que ya tienes (2007Q1..2024Q4) y la extendemos
fechas_all <- as.yearqtr(seq(from = as.Date("2007-01-01"), by = "quarter", length.out = T + h))
fechas_fc  <- tail(fechas_all, h)  # fechas pron√≥stico

# 4) Armar data.frames para exportar a CSV
# a) Pron√≥stico en diferencias
fc_diff_df <- data.frame(
  fecha_q = fechas_fc,
  as.data.frame(dhat_mean),
  check.names = FALSE
)

# b) Pron√≥stico en niveles (logs)
fc_level_df <- data.frame(
  fecha_q = fechas_fc,
  as.data.frame(lev_hat_mean),
  check.names = FALSE
)

# 5) Exportar a CSV (carpeta de trabajo actual)
write.csv(fc_diff_df,  "forecast_VARdiff_deltas.csv", row.names = FALSE)
write.csv(fc_level_df, "forecast_VARdiff_levels.csv", row.names = FALSE)
cat("‚úÖ CSV guardados: forecast_VARdiff_deltas.csv, forecast_VARdiff_levels.csv\n")

# 6) Gr√°ficos exportados (hist√≥rico + pron√≥stico) para 4 variables clave
plot_series_fc <- function(varname, file_png) {
  # Serie hist√≥rica (niveles log)
  hist_ts <- Y_ts[, varname]
  # Serie pronosticada (niveles log)
  fc_ts <- ts(lev_hat_mean[, varname],
              start = c(2007 + (T)/4, (T %% 4) + 1),  # arranque inmediatamente despu√©s
              frequency = 4)

  # Cierra y abre dispositivo PNG grande para evitar m√°rgenes
  while (!is.null(dev.list())) dev.off()
  png(file_png, width = 1400, height = 900, res = 150)
  par(mar = c(4,4,2,1))
  # Gr√°fico base del hist√≥rico
  plot(hist_ts, type = "l", lwd = 2, xlab = "Trimestre", ylab = "Log(nivel)",
       main = paste("Hist√≥rico y Forecast (niveles log) -", varname))
  # A√±adir pron√≥stico
  lines(window(fc_ts, start = tsp(fc_ts)[1]), lwd = 2, lty = 2)
  # L√≠mites (aprox.) de confianza en niveles
  lines(ts(lev_hat_low[, varname],
           start = start(fc_ts), frequency = 4), lty = 3)
  lines(ts(lev_hat_high[, varname],
           start = start(fc_ts), frequency = 4), lty = 3)
  legend("topleft", bty = "n",
         legend = c("Hist√≥rico", "Pron√≥stico (media)", "Banda baja", "Banda alta"),
         lwd = c(2,2,1,1), lty = c(1,2,3,3))
  dev.off()
  cat("‚úÖ Gr√°fico guardado:", file_png, "\n")
}

# Elige algunas variables representativas para la clase:
for (vn in c("L_PP","L_PIB","L_GG","L_IF")) {
  plot_series_fc(vn, paste0("FC_LEVELS_", vn, ".png"))
}

# 7) (Opcional) Si quieres pron√≥stico en diferencias (Œîlog) tambi√©n en PNG:
plot_series_fc_diff <- function(varname, file_png) {
  while (!is.null(dev.list())) dev.off()
  png(file_png, width = 1400, height = 900, res = 150)
  par(mar = c(4,4,2,1))
  plot(dY_ts[, varname], type = "l", lwd = 2, xlab = "Trimestre", ylab = "Œî log",
       main = paste("Hist√≥rico Œîlog y Forecast -", varname))
  lines(ts(dhat_mean[, varname],
           start = c(2007 + (T)/4, (T %% 4) + 1), frequency = 4), lwd = 2, lty = 2)
  lines(ts(dhat_low[, varname],
           start = c(2007 + (T)/4, (T %% 4) + 1), frequency = 4), lty = 3)
  lines(ts(dhat_high[, varname],
           start = c(2007 + (T)/4, (T %% 4) + 1), frequency = 4), lty = 3)
  legend("topleft", bty = "n",
         legend = c("Œîlog hist√≥rico", "Œîlog pron√≥stico (media)", "Banda baja", "Banda alta"),
         lwd = c(2,2,1,1), lty = c(1,2,3,3))
  dev.off()
  cat("‚úÖ Gr√°fico guardado:", file_png, "\n")
}

for (vn in c("L_PP","L_PIB","L_GG","L_IF")) {
  plot_series_fc_diff(vn, paste0("FC_DIFF_", vn, ".png"))
}

cat("\n=== FORECAST listo: CSV + PNG exportados ===\n")

```
